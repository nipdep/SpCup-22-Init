{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMqoDgCPMMD8"
      },
      "outputs": [],
      "source": [
        "! apt-get install libsox-fmt-all\n",
        "! apt-get install sox\n",
        "! pip install sox\n",
        "! pip install timm==0.4.5\n",
        "! pip install wget\n",
        "! pip install neptune-client\n",
        "! pip install torchcontrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pNwwsNVpMPz3"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import ast\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import torchaudio\n",
        "from typing import Dict, List, Union\n",
        "import neptune.new as neptune\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torchcontrib.optim import SWA\n",
        "basepath = os.path.dirname(os.path.dirname(sys.path[0]))\n",
        "sys.path.append(basepath)\n",
        "\n",
        "import dataloaderV2\n",
        "from ast_models import ASTModel\n",
        "from traintest import train, validate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_SAMPLE_DIR = \"_assets\"\n",
        "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"  # noqa: E501\n",
        "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
        "\n",
        "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"  # noqa: E501\n",
        "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
        "\n",
        "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"  # noqa: E501\n",
        "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
        "\n",
        "os.makedirs(_SAMPLE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "zAdF0RXBt7_0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _fetch_data():\n",
        "    uri = [\n",
        "        (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
        "        (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
        "        (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
        "    ]\n",
        "    for url, path in uri:\n",
        "        with open(path, \"wb\") as file_:\n",
        "            file_.write(requests.get(url).content)\n",
        "\n",
        "\n",
        "_fetch_data()"
      ],
      "metadata": {
        "id": "E045oOK9qkf6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_sample(path, resample=None):\n",
        "    effects = [[\"remix\", \"1\"]]\n",
        "    if resample:\n",
        "        effects.extend(\n",
        "            [\n",
        "                [\"lowpass\", f\"{resample // 2}\"],\n",
        "                [\"rate\", f\"{resample}\"],\n",
        "            ]\n",
        "        )\n",
        "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
        "\n",
        "def get_noise_sample(*, resample=None):\n",
        "    return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
        "\n",
        "noise, _ = get_noise_sample()"
      ],
      "metadata": {
        "id": "Fu5lH_-0uJDK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm1VwaDbMRJQ",
        "outputId": "581b2e70-5efe-4eca-edae-49225aa30535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/nipdep/sp-cup/e/SPCUP-45\n",
            "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ],
      "source": [
        "tracker = neptune.init(\n",
        "    project=\"nipdep/sp-cup\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
        ")  # your credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H-YNA_R9MSry"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgTYXbc6MUj4"
      },
      "outputs": [],
      "source": [
        "! wget \"https://www.dropbox.com/s/36yqmymkva2bwdi/spcup_2022_training_part1.zip?dl=1\" -c -O 'spcup_2022_training_part1.zip'\n",
        "! wget \"https://www.dropbox.com/s/wsmlthhri29fb79/spcup_2022_unseen.zip?dl=1\" -c -O 'spcup_2022_unseen.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bASRFdktMWoi"
      },
      "outputs": [],
      "source": [
        "!unzip \"./spcup_2022_training_part1.zip\" -d \"./spcup_2022_training/\"\n",
        "!unzip \"./spcup_2022_unseen.zip\" -d \"./spcup_2022_unseen/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ig3RS8vfMYOT"
      },
      "outputs": [],
      "source": [
        "!rm \"./spcup_2022_training_part1.zip\"\n",
        "!rm \"./spcup_2022_unseen.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_HriyDwwMZwv"
      },
      "outputs": [],
      "source": [
        "# create combined label df\n",
        "# combine two label sets\n",
        "df1 = pd.read_csv('./spcup_2022_training/spcup_2022_training_part1/labels.csv')\n",
        "df2 = pd.read_csv('./spcup_2022_unseen/spcup_2022_unseen/labels.csv')\n",
        "df3 = pd.concat([df1, df2]).sample(frac=1)\n",
        "\n",
        "df3.to_csv('./final_labels.csv', index=False)\n",
        "!rm './spcup_2022_unseen/spcup_2022_unseen/labels.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Dg9Co642MbMr"
      },
      "outputs": [],
      "source": [
        "# copy .wav files from unseen to all\n",
        "!cp -a \"./spcup_2022_unseen/spcup_2022_unseen/\". \"./spcup_2022_training/spcup_2022_training_part1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St-RWHTdMeGN"
      },
      "outputs": [],
      "source": [
        "! wget \"https://www.dropbox.com/s/ftkyvwxgr9wl7jf/spcup_2022_eval_part1.zip?dl=1\" -c -O \"./spcup_2022_eval_part1.zip\"\n",
        "! unzip \"./spcup_2022_eval_part1.zip\" -d \"./spcup_2022_eval_part1/\"\n",
        "!rm \"./spcup_2022_eval_part1.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tLHjp9OWMji_"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"./spcup_2022_training/spcup_2022_training_part1/\"\n",
        "IMAGE_PATH = './images'\n",
        "Labeled_dir = './final_labels.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SopQr5viMfM9"
      },
      "outputs": [],
      "source": [
        "label_df = pd.read_csv(Labeled_dir)\n",
        "# label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fl-mdr9IMmmJ"
      },
      "outputs": [],
      "source": [
        "label_df['wav_path'] = label_df['track'].map(lambda x: DATA_PATH+'/'+x)\n",
        "# label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BgFkirVkMoNn"
      },
      "outputs": [],
      "source": [
        "spect_type = 'mel'\n",
        "def save_and_record(wav_path):\n",
        "\n",
        "    image_name = wav_path.split('/')[-1].split('.')[0]\n",
        "    image_path = IMAGE_PATH+'/'+image_name+'.png'\n",
        "    return image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AjroiYg9MqvY"
      },
      "outputs": [],
      "source": [
        "col_map = {\n",
        "    'track': 'audio_id',\n",
        "    'algorithm': 'labels',\n",
        "    'wav_path': 'wav',\n",
        "    'image_path': 'image'\n",
        "}\n",
        "label_df.rename(col_map, axis=1, inplace=True)\n",
        "# label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VebanixqNETh"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(label_df, test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3FqqR-KkNEpd"
      },
      "outputs": [],
      "source": [
        "train_data_dict = {'data': train_df.to_dict('records')}\n",
        "with open(\"./final_train_data.json\", \"w\") as outfile:\n",
        "    json.dump(train_data_dict, outfile)\n",
        "\n",
        "test_data_dict = {'data': test_df.to_dict('records')}\n",
        "with open(\"./final_test_data.json\", \"w\") as outfile:\n",
        "    json.dump(test_data_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PCVEJQ3_NEy4"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('./spcup_2022_eval_part1/spcup_2022_eval_part1/labels_eval_part1.csv')\n",
        "test_df['track'] = test_df['track'].apply(lambda x: './spcup_2022_eval_part1/spcup_2022_eval_part1/'+x)\n",
        "# test_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AifEn5WQNJe8"
      },
      "outputs": [],
      "source": [
        "args = argparse.Namespace()\n",
        "\n",
        "args.data_train = './final_train_data.json'\n",
        "args.data_val = './final_test_data.json'\n",
        "args.data_test = test_df\n",
        "args.data_eval = '.json'\n",
        "args.n_class = 6\n",
        "args.model = 'ast'\n",
        "args.dataset = 'speechcommands'\n",
        "args.exp_dir = '.'\n",
        "args.lr = 0.001\n",
        "args.optim = 'adam'\n",
        "args.batch_size = 16\n",
        "args.num_workers =32\n",
        "args.n_epochs = 15\n",
        "args.lr_patience = 2\n",
        "args.n_print_steps = 100\n",
        "args.save_model = None # \n",
        "args.freqm = 0\n",
        "args.timem = 0\n",
        "args.mixup = 0\n",
        "args.bal = False\n",
        "args.fstride = 10\n",
        "args.tstride = 10\n",
        "args.imagenet_pretrain = True\n",
        "args.audioset_pretrain = False\n",
        "args.noise_level = 0.1\n",
        "args.optim_config = {\n",
        "        \"optimizer\": \"adam\", \n",
        "        \"amsgrad\": \"False\",\n",
        "        \"base_lr\": 0.0001,\n",
        "        \"lr_min\": 0.000005,\n",
        "        \"betas\": [0.9, 0.999],\n",
        "        \"weight_decay\": 0.0001,\n",
        "        \"scheduler\": \"cosine\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rasT-3apNJx7"
      },
      "outputs": [],
      "source": [
        "tracker[\"parameters\"] = vars(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9aQnrpOtNPWP"
      },
      "outputs": [],
      "source": [
        "# dataset spectrogram mean and std, used to normalize the input\n",
        "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
        "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
        "# if add noise for data augmentation, only use for speech commands\n",
        "noise = {'audioset': False, 'esc50': False, 'speechcommands':True}\n",
        "\n",
        "audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset], 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup, 'dataset': args.dataset, 'mode':'train', 'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1],\n",
        "                'noise':noise[args.dataset], 'noise_level': args.noise_level}\n",
        "val_audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset], 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset, 'mode':'evaluation', 'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1], 'noise':False}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFVinb2KNPpc",
        "outputId": "478ba50a-637c-4f7e-e123-0c69cd6b77e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "balanced sampler is not used\n",
            "---------------the train dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "now use noise augmentation\n",
            "number of classes is 6\n",
            "---------------the evaluation dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "number of classes is 6\n",
            "---------------the evaluation dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "number of classes is 6\n"
          ]
        }
      ],
      "source": [
        "if args.bal == 'bal':\n",
        "    print('balanced sampler is being used')\n",
        "    samples_weight = np.loadtxt(args.data_train[:-5]+'_weight.csv', delimiter=',')\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataloaderV2.AudiosetDataset(args.data_train, audio_conf=audio_conf),\n",
        "        batch_size=args.batch_size, sampler=sampler, num_workers=args.num_workers, pin_memory=True, shuffle=True)\n",
        "else:\n",
        "    print('balanced sampler is not used')\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataloaderV2.AudiosetDataset(args.data_train, audio_conf=audio_conf),\n",
        "        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataloaderV2.AudiosetDataset(args.data_val, audio_conf=val_audio_conf),\n",
        "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataloaderV2.AudioTestDataset(args.data_test, audio_conf=val_audio_conf),\n",
        "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii26TeN7NUSi",
        "outputId": "d72210cd-9099-43fe-9b17-1ade7d18cc8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=144\n"
          ]
        }
      ],
      "source": [
        "audio_model = ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
        "                                input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
        "                                audioset_pretrain=args.audioset_pretrain, model_size='base384')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rj_WrHIVNUlx"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    trn_loader: DataLoader,\n",
        "    model,\n",
        "    optim: Union[torch.optim.SGD, torch.optim.Adam],\n",
        "    device: torch.device,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    config: argparse.Namespace):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    \n",
        "    running_loss = 0\n",
        "    num_total = 0.0\n",
        "    train_acc, correct_train, target_count = 0, 0, 0\n",
        "    ii = 0\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # set objective (Loss) functions\n",
        "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
        "    # criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_x, batch_y in trn_loader:\n",
        "        batch_size = batch_x.size(0)\n",
        "        num_total += batch_size\n",
        "        ii += 1\n",
        "        batch_x = batch_x.to(device)\n",
        "        # batch_y = batch_y.view(-1).type(torch.long).to(device)\n",
        "        batch_y = batch_y.view(-1).type(torch.LongTensor).to(device)\n",
        "        with autocast():\n",
        "            batch_out = model(batch_x)#, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
        "            batch_loss = criterion(batch_out, batch_y)\n",
        "        running_loss += batch_loss.item() * batch_size\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(batch_loss).backward()\n",
        "        # batch_loss.backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        # optim.step()\n",
        "\n",
        "        if config.optim_config[\"scheduler\"] in [\"cosine\", \"keras_decay\"]:\n",
        "            scheduler.step()\n",
        "        elif scheduler is None:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"scheduler error, got:{}\".format(scheduler))\n",
        "        \n",
        "        # accuracy\n",
        "        _, predicted = torch.max(batch_out.data, 1)\n",
        "        target_count += batch_y.size(0)\n",
        "        correct_train += (batch_y == predicted).sum().item()\n",
        "        train_acc = (100 * correct_train) / target_count\n",
        "\n",
        "    running_loss /= num_total\n",
        "    return running_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HGa9UWiENYkn"
      },
      "outputs": [],
      "source": [
        "def eval_epoch(\n",
        "    trn_loader: DataLoader,\n",
        "    model,\n",
        "    device: torch.device,\n",
        "    config: argparse.Namespace):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    running_loss = 0\n",
        "    num_total = 0.0\n",
        "    val_acc, correct_train, target_count = 0, 0, 0\n",
        "    ii = 0\n",
        "    model.eval()\n",
        "\n",
        "    # set objective (Loss) functions\n",
        "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
        "    # criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_x, batch_y in trn_loader:\n",
        "        batch_size = batch_x.size(0)\n",
        "        num_total += batch_size\n",
        "        ii += 1\n",
        "        batch_x = batch_x.to(device)\n",
        "        # batch_y = batch_y.view(-1).type(torch.lo).to(device)\n",
        "        batch_y = batch_y.view(-1).type(torch.LongTensor).to(device)\n",
        "        with autocast():\n",
        "            batch_out = model(batch_x) #model(batch_x, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
        "            batch_loss = criterion(batch_out, batch_y)\n",
        "        running_loss += batch_loss.item() * batch_size\n",
        "        \n",
        "        # accuracy\n",
        "        _, predicted = torch.max(batch_out.data, 1)\n",
        "        target_count += batch_y.size(0)\n",
        "        correct_train += (batch_y == predicted).sum().item()\n",
        "        val_acc = (100 * correct_train) / target_count\n",
        "        \n",
        "    running_loss /= num_total\n",
        "    return running_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_ytdDwWpPdpr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SGDRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"SGD with restarts scheduler\"\"\"\n",
        "    def __init__(self, optimizer, T0, T_mul, eta_min, last_epoch=-1):\n",
        "        self.Ti = T0\n",
        "        self.T_mul = T_mul\n",
        "        self.eta_min = eta_min\n",
        "\n",
        "        self.last_restart = 0\n",
        "\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        T_cur = self.last_epoch - self.last_restart\n",
        "        if T_cur >= self.Ti:\n",
        "            self.last_restart = self.last_epoch\n",
        "            self.Ti = self.Ti * self.T_mul\n",
        "            T_cur = 0\n",
        "\n",
        "        return [\n",
        "            self.eta_min + (base_lr - self.eta_min) *\n",
        "            (1 + np.cos(np.pi * T_cur / self.Ti)) / 2\n",
        "            for base_lr in self.base_lrs\n",
        "        ]\n",
        "\n",
        "def _get_optimizer(model_parameters, optim_config):\n",
        "    \"\"\"Defines optimizer according to the given config\"\"\"\n",
        "    optimizer_name = optim_config['optimizer']\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model_parameters,\n",
        "                                    lr=optim_config['base_lr'],\n",
        "                                    momentum=optim_config['momentum'],\n",
        "                                    weight_decay=optim_config['weight_decay'],\n",
        "                                    nesterov=optim_config['nesterov'])\n",
        "    elif optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model_parameters,\n",
        "                                     lr=optim_config['base_lr'],\n",
        "                                     betas=optim_config['betas'],\n",
        "                                     weight_decay=optim_config['weight_decay'],\n",
        "                                     amsgrad=str_to_bool(\n",
        "                                         optim_config['amsgrad']))\n",
        "    else:\n",
        "        print('Un-known optimizer', optimizer_name)\n",
        "        sys.exit()\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def str_to_bool(val):\n",
        "    \"\"\"Convert a string representation of truth to true (1) or false (0).\n",
        "    Copied from the python implementation distutils.utils.strtobool\n",
        "\n",
        "    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n",
        "    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n",
        "    'val' is anything else.\n",
        "    >>> str_to_bool('YES')\n",
        "    1\n",
        "    >>> str_to_bool('FALSE')\n",
        "    0\n",
        "    \"\"\"\n",
        "    val = val.lower()\n",
        "    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n",
        "        return True\n",
        "    if val in ('n', 'no', 'f', 'false', 'off', '0'):\n",
        "        return False\n",
        "    raise ValueError('invalid truth value {}'.format(val))\n",
        "\n",
        "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
        "    \"\"\"Cosine Annealing for learning rate decay scheduler\"\"\"\n",
        "    return lr_min + (lr_max -\n",
        "                     lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "\n",
        "def keras_decay(step, decay=0.0001):\n",
        "    \"\"\"Learning rate decay in Keras-style\"\"\"\n",
        "    return 1. / (1. + decay * step)\n",
        "\n",
        "def _get_scheduler(optimizer, optim_config):\n",
        "    \"\"\"\n",
        "    Defines learning rate scheduler according to the given config\n",
        "    \"\"\"\n",
        "    if optim_config['scheduler'] == 'multistep':\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=optim_config['milestones'],\n",
        "            gamma=optim_config['lr_decay'])\n",
        "\n",
        "    elif optim_config['scheduler'] == 'sgdr':\n",
        "        scheduler = SGDRScheduler(optimizer, optim_config['T0'],\n",
        "                                  optim_config['Tmult'],\n",
        "                                  optim_config['lr_min'])\n",
        "\n",
        "    elif optim_config['scheduler'] == 'cosine':\n",
        "        total_steps = optim_config['epochs'] * \\\n",
        "            optim_config['steps_per_epoch']\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer,\n",
        "            lr_lambda=lambda step: cosine_annealing(\n",
        "                step,\n",
        "                total_steps,\n",
        "                1,  # since lr_lambda computes multiplicative factor\n",
        "                optim_config['lr_min'] / optim_config['base_lr']))\n",
        "\n",
        "    elif optim_config['scheduler'] == 'keras_decay':\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer, lr_lambda=lambda step: keras_decay(step))\n",
        "    else:\n",
        "        scheduler = None\n",
        "    return scheduler\n",
        "\n",
        "def create_optimizer(model_parameters, optim_config):\n",
        "    \"\"\"Defines an optimizer and a scheduler\"\"\"\n",
        "    optimizer = _get_optimizer(model_parameters, optim_config)\n",
        "    scheduler = _get_scheduler(optimizer, optim_config)\n",
        "    return optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rff6pl_NY3x",
        "outputId": "d5247611-e3ff-46ee-a146-9001ce75de19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=144\n",
            "Start training epoch000\n",
            "[0] Training Loss : 1.366928915229498 / Training Accuracy : 42.588235294117645 | Eval Loss : 1.081426298353407 / Eval Accuracy : 53.888888888888886\n",
            "Start training epoch001\n"
          ]
        }
      ],
      "source": [
        "# def main(args):\n",
        "\n",
        "# define database related paths\n",
        "# output_dir = Path(args.output_dir)\n",
        "# database_path = Path(config[\"database_path\"])\n",
        "# label_path = Path(config['label_path'])\n",
        "\n",
        "# set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: {}\".format(device))\n",
        "# if device == \"cpu\":\n",
        "#     raise ValueError(\"GPU not detected!\")\n",
        "\n",
        "# define model architecture\n",
        "# model_config = args.config[\"model_config\"]\n",
        "model = ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
        "                                input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
        "                                audioset_pretrain=args.audioset_pretrain, model_size='base384')\n",
        "model = model.to(device)\n",
        "\n",
        "# define dataloaders\n",
        "# trn_loader, eval_loader = get_loader(database_path, label_path, config)\n",
        "\n",
        "# get optimizer and scheduler\n",
        "optim_config = args.optim_config\n",
        "optim_config[\"epochs\"] = args.n_epochs\n",
        "optim_config[\"steps_per_epoch\"] = len(train_loader)\n",
        "optimizer, scheduler = create_optimizer(model.parameters(), optim_config)\n",
        "optimizer_swa = SWA(optimizer)\n",
        "\n",
        "# Training\n",
        "best_acc = 0.0\n",
        "for epoch in range(args.n_epochs):\n",
        "    print(\"Start training epoch{:03d}\".format(epoch))\n",
        "    training_loss, training_acc = train_epoch(train_loader, model, optimizer, device, scheduler, args)\n",
        "    eval_loss, eval_acc = eval_epoch(val_loader, model, device, args)\n",
        "    print(f'[{epoch}] Training Loss : {training_loss} / Training Accuracy : {training_acc} | Eval Loss : {eval_loss} / Eval Accuracy : {eval_acc}')\n",
        "    if eval_acc >= best_acc:\n",
        "        best_model = model.state_dict()\n",
        "\n",
        "y, pred = [], []\n",
        "model.load_state_dict(best_model)\n",
        "for batch_x, batch_y in train_loader:\n",
        "    batch_x = batch_x.to(device)\n",
        "    # batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
        "    with torch.no_grad():\n",
        "        batch_y = batch_y.view(-1).to(device)\n",
        "        _, batch_out = model(batch_x)\n",
        "    pred.extend(list(batch_out.cpu().numpy()))\n",
        "    y.extend(list(y.cpu().numpy()))\n",
        "\n",
        "# conf_metrics = confusion_matrix(pred, y)\n",
        "\n",
        "# return best_model, conf_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQdS9y1LUjpH"
      },
      "outputs": [],
      "source": [
        "# SETTING UP CODE TO RUN ON GPU\n",
        "gpu_id = 0\n",
        "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunY68J7UkV-"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataloaderV2.AudioTestDataset(args.data_test, audio_conf=val_audio_conf),\n",
        "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNj1QK3bUktR"
      },
      "outputs": [],
      "source": [
        "# load best model\n",
        "best_path = '/content/models/best_audio_model.pth'\n",
        "pred_model = ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
        "                                input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
        "                                audioset_pretrain=args.audioset_pretrain, model_size='base384')\n",
        "pred_model.load_state_dict(torch.load(best_path), strict=False)\n",
        "pred_model = pred_model.to(device)\n",
        "pred_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_arr = []\n",
        "for data in test_loader:\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        bt_preds = pred_model(data)\n",
        "    pred_arr.extend(list(bt_preds.cpu().numpy()))"
      ],
      "metadata": {
        "id": "pnAFBf67obkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_np = np.array(pred_arr)\n",
        "pred_labels = np.argmax(pred_np, axis=1)\n",
        "\n",
        "_df = pd.read_csv('./spcup_2022_eval_part1/spcup_2022_eval_part1/labels_eval_part1.csv')\n",
        "pred_df = pd.DataFrame({'track': _df['track'].values, 'label': pred_labels})\n",
        "pred_df.to_csv('./result.csv')\n",
        "\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "kCsbB8bmobyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracker.stop()"
      ],
      "metadata": {
        "id": "DZ-G98iOob8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ast_part2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}