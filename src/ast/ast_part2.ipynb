{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMqoDgCPMMD8"
      },
      "outputs": [],
      "source": [
        "! apt-get install libsox-fmt-all\n",
        "! apt-get install sox\n",
        "! pip install sox\n",
        "! pip install timm==0.4.5\n",
        "! pip install wget\n",
        "! pip install neptune-client\n",
        "! pip install torchcontrib\n",
        "! pip install neptune-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNwwsNVpMPz3"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import ast\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import torchaudio\n",
        "from typing import Dict, List, Union\n",
        "import neptune.new as neptune\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torchcontrib.optim import SWA\n",
        "basepath = os.path.dirname(os.path.dirname(sys.path[0]))\n",
        "sys.path.append(basepath)\n",
        "\n",
        "import dataloaderV2\n",
        "from ast_models import ASTModel\n",
        "# from traintest1 import train, validate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_SAMPLE_DIR = \"_assets\"\n",
        "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"  # noqa: E501\n",
        "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
        "\n",
        "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"  # noqa: E501\n",
        "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
        "\n",
        "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"  # noqa: E501\n",
        "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
        "\n",
        "os.makedirs(_SAMPLE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "zAdF0RXBt7_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _fetch_data():\n",
        "    uri = [\n",
        "        (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
        "        (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
        "        (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
        "    ]\n",
        "    for url, path in uri:\n",
        "        with open(path, \"wb\") as file_:\n",
        "            file_.write(requests.get(url).content)\n",
        "\n",
        "\n",
        "_fetch_data()"
      ],
      "metadata": {
        "id": "E045oOK9qkf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_sample(path, resample=None):\n",
        "    effects = [[\"remix\", \"1\"]]\n",
        "    if resample:\n",
        "        effects.extend(\n",
        "            [\n",
        "                [\"lowpass\", f\"{resample // 2}\"],\n",
        "                [\"rate\", f\"{resample}\"],\n",
        "            ]\n",
        "        )\n",
        "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
        "\n",
        "def get_noise_sample(*, resample=None):\n",
        "    return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
        "\n",
        "noise, _ = get_noise_sample()"
      ],
      "metadata": {
        "id": "Fu5lH_-0uJDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm1VwaDbMRJQ",
        "outputId": "babed719-7dee-4ccd-bb80-56bd5ede4e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/nipdep/sp-cup/e/SPCUP-49\n",
            "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ],
      "source": [
        "tracker = neptune.init(\n",
        "    project=\"nipdep/sp-cup\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
        ")  # your credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-YNA_R9MSry"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgTYXbc6MUj4"
      },
      "outputs": [],
      "source": [
        "! wget \"https://www.dropbox.com/s/36yqmymkva2bwdi/spcup_2022_training_part1.zip?dl=1\" -c -O 'spcup_2022_training_part1.zip'\n",
        "! wget \"https://www.dropbox.com/s/wsmlthhri29fb79/spcup_2022_unseen.zip?dl=1\" -c -O 'spcup_2022_unseen.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bASRFdktMWoi"
      },
      "outputs": [],
      "source": [
        "!unzip \"./spcup_2022_training_part1.zip\" -d \"./spcup_2022_training/\"\n",
        "!unzip \"./spcup_2022_unseen.zip\" -d \"./spcup_2022_unseen/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig3RS8vfMYOT"
      },
      "outputs": [],
      "source": [
        "!rm \"./spcup_2022_training_part1.zip\"\n",
        "!rm \"./spcup_2022_unseen.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HriyDwwMZwv"
      },
      "outputs": [],
      "source": [
        "# create combined label df\n",
        "# combine two label sets\n",
        "df1 = pd.read_csv('./spcup_2022_training/spcup_2022_training_part1/labels.csv')\n",
        "df2 = pd.read_csv('./spcup_2022_unseen/spcup_2022_unseen/labels.csv')\n",
        "df3 = pd.concat([df1, df2]).sample(frac=1)\n",
        "\n",
        "df3.to_csv('./final_labels.csv', index=False)\n",
        "!rm './spcup_2022_unseen/spcup_2022_unseen/labels.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg9Co642MbMr"
      },
      "outputs": [],
      "source": [
        "# copy .wav files from unseen to all\n",
        "!cp -a \"./spcup_2022_unseen/spcup_2022_unseen/\". \"./spcup_2022_training/spcup_2022_training_part1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St-RWHTdMeGN"
      },
      "outputs": [],
      "source": [
        "! wget \"https://www.dropbox.com/s/zylz07o2z0x308g/spcup_2022_eval_part2.zip?dl=1\" -c -O \"./spcup_2022_eval_part2.zip\"\n",
        "! unzip \"./spcup_2022_eval_part2.zip\" -d \"./spcup_2022_eval_part2/\"\n",
        "!rm \"./spcup_2022_eval_part2.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLHjp9OWMji_"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"./spcup_2022_training/spcup_2022_training_part1/\"\n",
        "IMAGE_PATH = './images'\n",
        "Labeled_dir = './final_labels.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SopQr5viMfM9"
      },
      "outputs": [],
      "source": [
        "label_df = pd.read_csv(Labeled_dir)\n",
        "# label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl-mdr9IMmmJ"
      },
      "outputs": [],
      "source": [
        "label_df['wav_path'] = label_df['track'].map(lambda x: DATA_PATH+'/'+x)\n",
        "# label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgFkirVkMoNn"
      },
      "outputs": [],
      "source": [
        "spect_type = 'mel'\n",
        "def save_and_record(wav_path):\n",
        "\n",
        "    image_name = wav_path.split('/')[-1].split('.')[0]\n",
        "    image_path = IMAGE_PATH+'/'+image_name+'.png'\n",
        "    return image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjroiYg9MqvY"
      },
      "outputs": [],
      "source": [
        "col_map = {\n",
        "    'track': 'audio_id',\n",
        "    'algorithm': 'labels',\n",
        "    'wav_path': 'wav',\n",
        "    'image_path': 'image'\n",
        "}\n",
        "label_df.rename(col_map, axis=1, inplace=True)\n",
        "# label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VebanixqNETh"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(label_df, test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FqqR-KkNEpd"
      },
      "outputs": [],
      "source": [
        "train_data_dict = {'data': train_df.to_dict('records')}\n",
        "with open(\"./final_train_data.json\", \"w\") as outfile:\n",
        "    json.dump(train_data_dict, outfile)\n",
        "\n",
        "test_data_dict = {'data': test_df.to_dict('records')}\n",
        "with open(\"./final_test_data.json\", \"w\") as outfile:\n",
        "    json.dump(test_data_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCVEJQ3_NEy4"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('./spcup_2022_eval_part2/spcup_2022_eval_part2/labels_eval_part2.csv')\n",
        "test_df['track'] = test_df['track'].apply(lambda x: './spcup_2022_eval_part2/spcup_2022_eval_part2/'+x)\n",
        "# test_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AifEn5WQNJe8"
      },
      "outputs": [],
      "source": [
        "args = argparse.Namespace()\n",
        "\n",
        "args.data_train = './final_train_data.json'\n",
        "args.data_val = './final_test_data.json'\n",
        "args.data_test = test_df\n",
        "args.data_eval = '.json'\n",
        "args.n_class = 6\n",
        "args.model = 'ast'\n",
        "args.dataset = 'speechcommands'\n",
        "args.exp_dir = '.'\n",
        "args.lr = 0.001\n",
        "args.optim = 'adam'\n",
        "args.batch_size = 16\n",
        "args.num_workers =32\n",
        "args.n_epochs = 100\n",
        "args.lr_patience = 2\n",
        "args.n_print_steps = 100\n",
        "args.save_model = None # \n",
        "args.freqm = 0\n",
        "args.timem = 0\n",
        "args.mixup = 0\n",
        "args.bal = False\n",
        "args.fstride = 10\n",
        "args.tstride = 10\n",
        "args.imagenet_pretrain = True\n",
        "args.audioset_pretrain = False\n",
        "args.noise_level = 0.1\n",
        "args.optim_config = {\n",
        "        \"optimizer\": \"adam\", \n",
        "        \"amsgrad\": \"False\",\n",
        "        \"base_lr\": 0.0001,\n",
        "        \"lr_min\": 0.000005,\n",
        "        \"betas\": [0.9, 0.999],\n",
        "        \"weight_decay\": 0.0001,\n",
        "        \"scheduler\": \"cosine\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rasT-3apNJx7"
      },
      "outputs": [],
      "source": [
        "tracker[\"parameters\"] = vars(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aQnrpOtNPWP"
      },
      "outputs": [],
      "source": [
        "# dataset spectrogram mean and std, used to normalize the input\n",
        "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
        "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
        "# if add noise for data augmentation, only use for speech commands\n",
        "noise = {'audioset': False, 'esc50': False, 'speechcommands':True}\n",
        "\n",
        "audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset], 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup, 'dataset': args.dataset, 'mode':'train', 'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1],\n",
        "                'noise':noise[args.dataset], 'noise_level': args.noise_level}\n",
        "val_audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset], 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset, 'mode':'evaluation', 'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1], 'noise':False}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFVinb2KNPpc",
        "outputId": "21b15079-294f-41e8-eb6e-ed6180d3e6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "balanced sampler is not used\n",
            "---------------the train dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "now use noise augmentation\n",
            "number of classes is 6\n",
            "---------------the evaluation dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "number of classes is 6\n",
            "---------------the evaluation dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "number of classes is 6\n"
          ]
        }
      ],
      "source": [
        "if args.bal == 'bal':\n",
        "    print('balanced sampler is being used')\n",
        "    samples_weight = np.loadtxt(args.data_train[:-5]+'_weight.csv', delimiter=',')\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataloaderV2.AudiosetDataset(args.data_train, audio_conf=audio_conf),\n",
        "        batch_size=args.batch_size, sampler=sampler, num_workers=args.num_workers, pin_memory=True, shuffle=True)\n",
        "else:\n",
        "    print('balanced sampler is not used')\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataloaderV2.AudiosetDataset(args.data_train, audio_conf=audio_conf),\n",
        "        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataloaderV2.AudiosetDataset(args.data_val, audio_conf=val_audio_conf),\n",
        "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataloaderV2.AudioTestDataset(args.data_test, audio_conf=val_audio_conf),\n",
        "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii26TeN7NUSi",
        "outputId": "dc0c056d-d566-4b9c-80cf-c97a4570f668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\" to ../../pretrained_models/hub/checkpoints/deit_base_distilled_patch16_384-d0272ac0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frequncey stride=10, time stride=10\n",
            "number of patches=144\n"
          ]
        }
      ],
      "source": [
        "audio_model = ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
        "                                input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
        "                                audioset_pretrain=args.audioset_pretrain, model_size='base384')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj_WrHIVNUlx"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    trn_loader: DataLoader,\n",
        "    model,\n",
        "    optim: Union[torch.optim.SGD, torch.optim.Adam],\n",
        "    device: torch.device,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    config: argparse.Namespace):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    \n",
        "    running_loss = 0\n",
        "    num_total = 0.0\n",
        "    train_acc, correct_train, target_count = 0, 0, 0\n",
        "    ii = 0\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # set objective (Loss) functions\n",
        "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
        "    # criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_x, batch_y in trn_loader:\n",
        "        batch_size = batch_x.size(0)\n",
        "        num_total += batch_size\n",
        "        ii += 1\n",
        "        batch_x = batch_x.to(device)\n",
        "        # batch_y = batch_y.view(-1).type(torch.long).to(device)\n",
        "        batch_y = batch_y.view(-1).type(torch.LongTensor).to(device)\n",
        "        with autocast():\n",
        "            batch_out = model(batch_x)#, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
        "            batch_loss = criterion(batch_out, batch_y)\n",
        "        running_loss += batch_loss.item() * batch_size\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(batch_loss).backward()\n",
        "        # batch_loss.backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        # optim.step()\n",
        "\n",
        "        if config.optim_config[\"scheduler\"] in [\"cosine\", \"keras_decay\"]:\n",
        "            scheduler.step()\n",
        "        elif scheduler is None:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"scheduler error, got:{}\".format(scheduler))\n",
        "        \n",
        "        # accuracy\n",
        "        _, predicted = torch.max(batch_out.data, 1)\n",
        "        target_count += batch_y.size(0)\n",
        "        correct_train += (batch_y == predicted).sum().item()\n",
        "        train_acc = (100 * correct_train) / target_count\n",
        "\n",
        "    running_loss /= num_total\n",
        "    return running_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGa9UWiENYkn"
      },
      "outputs": [],
      "source": [
        "def eval_epoch(\n",
        "    trn_loader: DataLoader,\n",
        "    model,\n",
        "    device: torch.device,\n",
        "    config: argparse.Namespace):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    running_loss = 0\n",
        "    num_total = 0.0\n",
        "    val_acc, correct_train, target_count = 0, 0, 0\n",
        "    ii = 0\n",
        "    model.eval()\n",
        "\n",
        "    # set objective (Loss) functions\n",
        "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
        "    # criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_x, batch_y in trn_loader:\n",
        "        batch_size = batch_x.size(0)\n",
        "        num_total += batch_size\n",
        "        ii += 1\n",
        "        batch_x = batch_x.to(device)\n",
        "        # batch_y = batch_y.view(-1).type(torch.lo).to(device)\n",
        "        batch_y = batch_y.view(-1).type(torch.LongTensor).to(device)\n",
        "        with autocast():\n",
        "            batch_out = model(batch_x) #model(batch_x, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
        "            batch_loss = criterion(batch_out, batch_y)\n",
        "        running_loss += batch_loss.item() * batch_size\n",
        "        \n",
        "        # accuracy\n",
        "        _, predicted = torch.max(batch_out.data, 1)\n",
        "        target_count += batch_y.size(0)\n",
        "        correct_train += (batch_y == predicted).sum().item()\n",
        "        val_acc = (100 * correct_train) / target_count\n",
        "        \n",
        "    running_loss /= num_total\n",
        "    return running_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ytdDwWpPdpr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SGDRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"SGD with restarts scheduler\"\"\"\n",
        "    def __init__(self, optimizer, T0, T_mul, eta_min, last_epoch=-1):\n",
        "        self.Ti = T0\n",
        "        self.T_mul = T_mul\n",
        "        self.eta_min = eta_min\n",
        "\n",
        "        self.last_restart = 0\n",
        "\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        T_cur = self.last_epoch - self.last_restart\n",
        "        if T_cur >= self.Ti:\n",
        "            self.last_restart = self.last_epoch\n",
        "            self.Ti = self.Ti * self.T_mul\n",
        "            T_cur = 0\n",
        "\n",
        "        return [\n",
        "            self.eta_min + (base_lr - self.eta_min) *\n",
        "            (1 + np.cos(np.pi * T_cur / self.Ti)) / 2\n",
        "            for base_lr in self.base_lrs\n",
        "        ]\n",
        "\n",
        "def _get_optimizer(model_parameters, optim_config):\n",
        "    \"\"\"Defines optimizer according to the given config\"\"\"\n",
        "    optimizer_name = optim_config['optimizer']\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model_parameters,\n",
        "                                    lr=optim_config['base_lr'],\n",
        "                                    momentum=optim_config['momentum'],\n",
        "                                    weight_decay=optim_config['weight_decay'],\n",
        "                                    nesterov=optim_config['nesterov'])\n",
        "    elif optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model_parameters,\n",
        "                                     lr=optim_config['base_lr'],\n",
        "                                     betas=optim_config['betas'],\n",
        "                                     weight_decay=optim_config['weight_decay'],\n",
        "                                     amsgrad=str_to_bool(\n",
        "                                         optim_config['amsgrad']))\n",
        "    else:\n",
        "        print('Un-known optimizer', optimizer_name)\n",
        "        sys.exit()\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def str_to_bool(val):\n",
        "    \"\"\"Convert a string representation of truth to true (1) or false (0).\n",
        "    Copied from the python implementation distutils.utils.strtobool\n",
        "\n",
        "    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n",
        "    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n",
        "    'val' is anything else.\n",
        "    >>> str_to_bool('YES')\n",
        "    1\n",
        "    >>> str_to_bool('FALSE')\n",
        "    0\n",
        "    \"\"\"\n",
        "    val = val.lower()\n",
        "    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n",
        "        return True\n",
        "    if val in ('n', 'no', 'f', 'false', 'off', '0'):\n",
        "        return False\n",
        "    raise ValueError('invalid truth value {}'.format(val))\n",
        "\n",
        "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
        "    \"\"\"Cosine Annealing for learning rate decay scheduler\"\"\"\n",
        "    return lr_min + (lr_max -\n",
        "                     lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "\n",
        "def keras_decay(step, decay=0.0001):\n",
        "    \"\"\"Learning rate decay in Keras-style\"\"\"\n",
        "    return 1. / (1. + decay * step)\n",
        "\n",
        "def _get_scheduler(optimizer, optim_config):\n",
        "    \"\"\"\n",
        "    Defines learning rate scheduler according to the given config\n",
        "    \"\"\"\n",
        "    if optim_config['scheduler'] == 'multistep':\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=optim_config['milestones'],\n",
        "            gamma=optim_config['lr_decay'])\n",
        "\n",
        "    elif optim_config['scheduler'] == 'sgdr':\n",
        "        scheduler = SGDRScheduler(optimizer, optim_config['T0'],\n",
        "                                  optim_config['Tmult'],\n",
        "                                  optim_config['lr_min'])\n",
        "\n",
        "    elif optim_config['scheduler'] == 'cosine':\n",
        "        total_steps = optim_config['epochs'] * \\\n",
        "            optim_config['steps_per_epoch']\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer,\n",
        "            lr_lambda=lambda step: cosine_annealing(\n",
        "                step,\n",
        "                total_steps,\n",
        "                1,  # since lr_lambda computes multiplicative factor\n",
        "                optim_config['lr_min'] / optim_config['base_lr']))\n",
        "\n",
        "    elif optim_config['scheduler'] == 'keras_decay':\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer, lr_lambda=lambda step: keras_decay(step))\n",
        "    else:\n",
        "        scheduler = None\n",
        "    return scheduler\n",
        "\n",
        "def create_optimizer(model_parameters, optim_config):\n",
        "    \"\"\"Defines an optimizer and a scheduler\"\"\"\n",
        "    optimizer = _get_optimizer(model_parameters, optim_config)\n",
        "    scheduler = _get_scheduler(optimizer, optim_config)\n",
        "    return optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "0Rff6pl_NY3x",
        "outputId": "bc00750a-e9ff-4343-e71e-347319033261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=144\n",
            "Start training epoch000\n",
            "[0] Training Loss : 1.0311412952927983 / Training Accuracy : 56.64705882352941 | Eval Loss : 0.7527351485358345 / Eval Accuracy : 66.55555555555556\n",
            "Start training epoch001\n",
            "[1] Training Loss : 0.5977663866211387 / Training Accuracy : 75.52941176470588 | Eval Loss : 0.47003057559331257 / Eval Accuracy : 81.33333333333333\n",
            "Start training epoch002\n",
            "[2] Training Loss : 0.46483842709485224 / Training Accuracy : 81.86274509803921 | Eval Loss : 0.438862689336141 / Eval Accuracy : 80.88888888888889\n",
            "Start training epoch003\n",
            "[3] Training Loss : 0.37351549565792086 / Training Accuracy : 84.56862745098039 | Eval Loss : 0.40318898995717367 / Eval Accuracy : 84.0\n",
            "Start training epoch004\n",
            "[4] Training Loss : 0.32366487568499996 / Training Accuracy : 87.41176470588235 | Eval Loss : 0.28445544878641765 / Eval Accuracy : 88.77777777777777\n",
            "Start training epoch005\n",
            "[5] Training Loss : 0.2821522646324307 / Training Accuracy : 89.15686274509804 | Eval Loss : 0.3433414724138048 / Eval Accuracy : 86.33333333333333\n",
            "Start training epoch006\n",
            "[6] Training Loss : 0.24416753612312617 / Training Accuracy : 90.88235294117646 | Eval Loss : 0.26927432854970296 / Eval Accuracy : 90.33333333333333\n",
            "Start training epoch007\n",
            "[7] Training Loss : 0.21321723760343064 / Training Accuracy : 91.86274509803921 | Eval Loss : 0.19902243653933208 / Eval Accuracy : 92.11111111111111\n",
            "Start training epoch008\n",
            "[8] Training Loss : 0.16007971923140918 / Training Accuracy : 94.17647058823529 | Eval Loss : 0.2565157286326091 / Eval Accuracy : 90.44444444444444\n",
            "Start training epoch009\n",
            "[9] Training Loss : 0.14348318866654938 / Training Accuracy : 94.52941176470588 | Eval Loss : 0.19147875401708814 / Eval Accuracy : 92.0\n",
            "Start training epoch010\n",
            "[10] Training Loss : 0.11205859256579595 / Training Accuracy : 95.80392156862744 | Eval Loss : 0.1788937258720398 / Eval Accuracy : 93.22222222222223\n",
            "Start training epoch011\n",
            "[11] Training Loss : 0.10062805406018799 / Training Accuracy : 96.25490196078431 | Eval Loss : 0.20100676470332676 / Eval Accuracy : 92.11111111111111\n",
            "Start training epoch012\n",
            "[12] Training Loss : 0.07949198253160598 / Training Accuracy : 97.15686274509804 | Eval Loss : 0.17839024411307441 / Eval Accuracy : 94.44444444444444\n",
            "Start training epoch013\n",
            "[13] Training Loss : 0.05765222862728086 / Training Accuracy : 97.88235294117646 | Eval Loss : 0.18368529319763183 / Eval Accuracy : 93.11111111111111\n",
            "Start training epoch014\n",
            "[14] Training Loss : 0.06256724441757279 / Training Accuracy : 97.72549019607843 | Eval Loss : 0.1336548324426015 / Eval Accuracy : 95.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9d7690c42dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ],
      "source": [
        "# def main(args):\n",
        "\n",
        "# define database related paths\n",
        "# output_dir = Path(args.output_dir)\n",
        "# database_path = Path(config[\"database_path\"])\n",
        "# label_path = Path(config['label_path'])\n",
        "\n",
        "# set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: {}\".format(device))\n",
        "# if device == \"cpu\":\n",
        "#     raise ValueError(\"GPU not detected!\")\n",
        "\n",
        "# define model architecture\n",
        "# model_config = args.config[\"model_config\"]\n",
        "model = ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
        "                                input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
        "                                audioset_pretrain=args.audioset_pretrain, model_size='base384')\n",
        "model = model.to(device)\n",
        "\n",
        "# define dataloaders\n",
        "# trn_loader, eval_loader = get_loader(database_path, label_path, config)\n",
        "\n",
        "# get optimizer and scheduler\n",
        "optim_config = args.optim_config\n",
        "optim_config[\"epochs\"] = args.n_epochs\n",
        "optim_config[\"steps_per_epoch\"] = len(train_loader)\n",
        "optimizer, scheduler = create_optimizer(model.parameters(), optim_config)\n",
        "optimizer_swa = SWA(optimizer)\n",
        "\n",
        "# Training\n",
        "best_acc = 0.0\n",
        "for epoch in range(args.n_epochs):\n",
        "    print(\"Start training epoch{:03d}\".format(epoch))\n",
        "    training_loss, training_acc = train_epoch(train_loader, model, optimizer, device, scheduler, args)\n",
        "    eval_loss, eval_acc = eval_epoch(val_loader, model, device, args)\n",
        "    tracker['train/loss'].log(training_loss)\n",
        "    tracker['train/acc'].log(training_acc)\n",
        "    tracker['eval/loss'].log(eval_loss)\n",
        "    tracker['eval/acc'].log(eval_acc)\n",
        "    print(f'[{epoch}] Training Loss : {training_loss} / Training Accuracy : {training_acc} | Eval Loss : {eval_loss} / Eval Accuracy : {eval_acc}')\n",
        "\n",
        "    if eval_acc > best_acc: \n",
        "            torch.save(model.state_dict(), \"./best_audio_model.pth\")\n",
        "\n",
        "y, pred = [], []\n",
        "model.load_state_dict(best_model)\n",
        "for batch_x, batch_y in train_loader:\n",
        "    batch_x = batch_x.to(device)\n",
        "    # batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
        "    with torch.no_grad():\n",
        "        batch_y = batch_y.view(-1).to(device)\n",
        "        _, batch_out = model(batch_x)\n",
        "    pred.extend(list(batch_out.cpu().numpy()))\n",
        "    y.extend(list(y.cpu().numpy()))\n",
        "\n",
        "# conf_metrics = confusion_matrix(pred, y)\n",
        "\n",
        "# return best_model, conf_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQdS9y1LUjpH"
      },
      "outputs": [],
      "source": [
        "# SETTING UP CODE TO RUN ON GPU\n",
        "gpu_id = 0\n",
        "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunY68J7UkV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d190e345-b5c0-4aae-a3bc-516c5ee1b21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------the evaluation dataloader---------------\n",
            "now using following mask: 0 freq, 0 time\n",
            "now using mix-up with rate 0.000000\n",
            "now process speechcommands\n",
            "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
            "number of classes is 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataloaderV2.AudioTestDataset(args.data_test, audio_conf=val_audio_conf),\n",
        "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNj1QK3bUktR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868eb378-f871-466f-b2c9-f21f757b5ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=144\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ASTModel(\n",
              "  (v): DistilledVisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (pre_logits): Identity()\n",
              "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              "    (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=768, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# load best model\n",
        "best_path = './best_audio_model.pth'\n",
        "pred_model = ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
        "                                input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
        "                                audioset_pretrain=args.audioset_pretrain, model_size='base384')\n",
        "pred_model.load_state_dict(torch.load(best_path), strict=False)\n",
        "pred_model = pred_model.to(device)\n",
        "pred_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_arr = []\n",
        "for data in test_loader:\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        bt_preds = pred_model(data)\n",
        "    pred_arr.extend(list(bt_preds.cpu().numpy()))"
      ],
      "metadata": {
        "id": "pnAFBf67obkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d55f3fe-8f50-4a97-b130-bbed7ade5f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_np = np.array(pred_arr)\n",
        "pred_labels = np.argmax(pred_np, axis=1)\n",
        "\n",
        "_df = pd.read_csv('./spcup_2022_eval_part2/spcup_2022_eval_part2/labels_eval_part2.csv')\n",
        "pred_df = pd.DataFrame({'track': _df['track'].values, 'label': pred_labels})\n",
        "pred_df.to_csv('./result.csv')\n",
        "\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "kCsbB8bmobyV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9124d93f-5a75-407d-ae44-61cdd878bac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  track  label\n",
              "0  36d06279048026b1059236c4b5024f78.wav      0\n",
              "1  49550c0090367cd3bf8353b7c4a8f62a.wav      1\n",
              "2  b336ac2a2b8cf99ff922bbd3d29a68a3.wav      1\n",
              "3  aac54fb7d3a8ba2991b6222100560fd4.wav      4\n",
              "4  9e5e48d5b4b147bb50dd4bf851b182c5.wav      3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9e02580-bc7d-419e-9d6e-3790006dfd4e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>track</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36d06279048026b1059236c4b5024f78.wav</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49550c0090367cd3bf8353b7c4a8f62a.wav</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b336ac2a2b8cf99ff922bbd3d29a68a3.wav</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aac54fb7d3a8ba2991b6222100560fd4.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9e5e48d5b4b147bb50dd4bf851b182c5.wav</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9e02580-bc7d-419e-9d6e-3790006dfd4e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a9e02580-bc7d-419e-9d6e-3790006dfd4e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a9e02580-bc7d-419e-9d6e-3790006dfd4e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracker.stop()"
      ],
      "metadata": {
        "id": "DZ-G98iOob8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ast_part2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}