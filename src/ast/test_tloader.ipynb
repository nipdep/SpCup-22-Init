{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import wave\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00050dd7458cf08e594c797930696bce.wav</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00070e7c531000d3dddc735d107275a9.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000f0711027a69b7f3886c2dbcb7d41f.wav</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001e28e66dee24408aaf3480dfb95fbe.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001eee950f60613869544b72cd48fe97.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  track  algorithm\n",
       "0  00050dd7458cf08e594c797930696bce.wav          4\n",
       "1  00070e7c531000d3dddc735d107275a9.wav          2\n",
       "2  000f0711027a69b7f3886c2dbcb7d41f.wav          3\n",
       "3  001e28e66dee24408aaf3480dfb95fbe.wav          1\n",
       "4  001eee950f60613869544b72cd48fe97.wav          2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../data/labels.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/spcup_2022_training_part1/00050dd74...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/spcup_2022_training_part1/00070e7c5...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/spcup_2022_training_part1/000f07110...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/spcup_2022_training_part1/001e28e66...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/spcup_2022_training_part1/001eee950...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               track  algorithm\n",
       "0  ../../data/spcup_2022_training_part1/00050dd74...          4\n",
       "1  ../../data/spcup_2022_training_part1/00070e7c5...          2\n",
       "2  ../../data/spcup_2022_training_part1/000f07110...          3\n",
       "3  ../../data/spcup_2022_training_part1/001e28e66...          1\n",
       "4  ../../data/spcup_2022_training_part1/001eee950...          2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = test_df.copy()\n",
    "_df['track'] = _df['track'].apply(lambda x: '../../data/spcup_2022_training_part1/'+x)\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "args.data_train = './final_train_data.json'\n",
    "args.data_val = './final_test_data.json'\n",
    "args.data_test = _df\n",
    "args.data_eval = '.json'\n",
    "args.n_class = 6\n",
    "args.model = 'ast'\n",
    "args.dataset = 'speechcommands'\n",
    "args.exp_dir = '.'\n",
    "args.lr = 0.001\n",
    "args.optim = 'adam'\n",
    "args.batch_size = 4\n",
    "args.num_workers =4\n",
    "args.n_epochs = 3\n",
    "args.lr_patience = 2\n",
    "args.n_print_steps = 100\n",
    "args.save_model = None # \n",
    "args.freqm = 0\n",
    "args.timem = 0\n",
    "args.mixup = 0\n",
    "args.bal = False\n",
    "args.fstride = 10\n",
    "args.tstride = 10\n",
    "args.imagenet_pretrain = True\n",
    "args.audioset_pretrain = True\n",
    "args.noise_level = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset spectrogram mean and std, used to normalize the input\n",
    "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "# if add noise for data augmentation, only use for speech commands\n",
    "noise = {'audioset': False, 'esc50': False, 'speechcommands':True}\n",
    "\n",
    "val_audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset], 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset, 'mode':'evaluation', 'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1], 'noise':False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTestDataset(Dataset):\n",
    "    def __init__(self, dataset_df, audio_conf):\n",
    "        \"\"\"\n",
    "        Dataset that manages audio recordings\n",
    "        :param audio_conf: Dictionary containing the audio loading and preprocessing settings\n",
    "        :param dataset_json_file\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = list(dataset_df.loc[:, ['track']].values)\n",
    "        self.audio_conf = audio_conf\n",
    "        print(\n",
    "            '---------------the {:s} dataloader---------------'.format(self.audio_conf.get('mode')))\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.freqm = self.audio_conf.get('freqm')\n",
    "        self.timem = self.audio_conf.get('timem')\n",
    "        print('now using following mask: {:d} freq, {:d} time'.format(\n",
    "            self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        self.mixup = self.audio_conf.get('mixup')\n",
    "        print('now using mix-up with rate {:f}'.format(self.mixup))\n",
    "        self.dataset = self.audio_conf.get('dataset')\n",
    "        print('now process ' + self.dataset)\n",
    "        # dataset spectrogram mean and std, used to normalize the input\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        # skip_norm is a flag that if you want to skip normalization to compute the normalization stats using src/get_norm_stats.py, if Ture, input normalization will be skipped for correctly calculating the stats.\n",
    "        # set it as True ONLY when you are getting the normalization stats.\n",
    "        self.skip_norm = self.audio_conf.get(\n",
    "            'skip_norm') if self.audio_conf.get('skip_norm') else False\n",
    "        if self.skip_norm:\n",
    "            print(\n",
    "                'now skip normalization (use it ONLY when you are computing the normalization stats).')\n",
    "        else:\n",
    "            print('use dataset mean {:.3f} and std {:.3f} to normalize the input.'.format(\n",
    "                self.norm_mean, self.norm_std))\n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise')\n",
    "        self.noise_lvl = self.audio_conf.get('noise_level')\n",
    "        if self.noise == True:\n",
    "            print('now use noise augmentation')\n",
    "\n",
    "        # self.index_dict = make_index_dict(label_csv)\n",
    "        self.label_num = 6  # len(self.index_dict)\n",
    "        print('number of classes is {:d}'.format(self.label_num))\n",
    "\n",
    "    def _wav2fbank(self, filename, filename2=None):\n",
    "        # mixup\n",
    "        print(filename)\n",
    "        if filename2 == None:\n",
    "            waveform, sr = torchaudio.load(filename)\n",
    "            waveform = waveform - waveform.mean()\n",
    "        # mixup\n",
    "        else:\n",
    "            waveform1, sr = torchaudio.load(filename)\n",
    "            waveform2, _ = torchaudio.load(filename2)\n",
    "\n",
    "            waveform1 = waveform1 - waveform1.mean()\n",
    "            waveform2 = waveform2 - waveform2.mean()\n",
    "\n",
    "            if waveform1.shape[1] != waveform2.shape[1]:\n",
    "                if waveform1.shape[1] > waveform2.shape[1]:\n",
    "                    # padding\n",
    "                    temp_wav = torch.zeros(1, waveform1.shape[1])\n",
    "                    temp_wav[0, 0:waveform2.shape[1]] = waveform2\n",
    "                    waveform2 = temp_wav\n",
    "                else:\n",
    "                    # cutting\n",
    "                    waveform2 = waveform2[0, 0:waveform1.shape[1]]\n",
    "\n",
    "            # sample lambda from uniform distribution\n",
    "            #mix_lambda = random.random()\n",
    "            # sample lambda from beta distribtion\n",
    "            mix_lambda = np.random.beta(10, 10)\n",
    "\n",
    "            mix_waveform = mix_lambda * waveform1 + \\\n",
    "                (1 - mix_lambda) * waveform2\n",
    "            waveform = mix_waveform - mix_waveform.mean()\n",
    "\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "                                                  window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "\n",
    "        target_length = self.audio_conf.get('target_length')\n",
    "        n_frames = fbank.shape[0]\n",
    "\n",
    "        p = target_length - n_frames\n",
    "\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "\n",
    "        if filename2 == None:\n",
    "            return fbank, 0\n",
    "        else:\n",
    "            return fbank, mix_lambda\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns: image, audio, nframes\n",
    "        where image is a FloatTensor of size (3, H, W)\n",
    "        audio is a FloatTensor of size (N_freq, N_frames) for spectrogram, or (N_frames) for waveform\n",
    "        nframes is an integer\n",
    "        \"\"\"\n",
    "        # do mix-up for this sample (controlled by the given mixup rate)\n",
    "        if random.random() < self.mixup:\n",
    "            datum = self.data[index]\n",
    "            # find another sample to mix, also do balance sampling\n",
    "            # sample the other sample from the multinomial distribution, will make the performance worse\n",
    "            # mix_sample_idx = np.random.choice(len(self.data), p=self.sample_weight_file)\n",
    "            # sample the other sample from the uniform distribution\n",
    "            mix_sample_idx = random.randint(0, len(self.data)-1)\n",
    "            mix_datum = self.data[mix_sample_idx]\n",
    "            # get the mixed fbank\n",
    "            fbank, mix_lambda = self._wav2fbank(datum, mix_datum)\n",
    "            # initialize the label\n",
    "            # label_indices = np.zeros(self.label_num)\n",
    "            # add sample 1 labels\n",
    "            # for label_str in datum['labels'].split(','):\n",
    "            #     label_indices[int(self.index_dict[label_str])] += mix_lambda\n",
    "            # label_indices[datum['labels']] += mix_datum\n",
    "            # add sample 2 labels\n",
    "            # for label_str in mix_datum['labels'].split(','):\n",
    "            #     label_indices[int(self.index_dict[label_str])] += 1.0-mix_lambda\n",
    "            # label_indices[mix_datum['labels']] += (1.0 - mix_lambda)\n",
    "            # label_indices = torch.FloatTensor(label_indices)\n",
    "        # if not do mixup\n",
    "        else:\n",
    "            datum = self.data[index]\n",
    "            # label_indices = np.zeros(self.label_num)\n",
    "            fbank, mix_lambda = self._wav2fbank(datum)\n",
    "            # for label_str in datum['labels'].split(','):\n",
    "            #     label_indices[int(self.index_dict[label_str])] = 1.0\n",
    "            # label_indices[datum['labels']] += 1.0\n",
    "\n",
    "            # label_indices = torch.FloatTensor(label_indices)\n",
    "\n",
    "        # SpecAug, not do for eval set\n",
    "        # freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "        # timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "        # fbank = torch.transpose(fbank, 0, 1)\n",
    "        # if self.freqm != 0:\n",
    "        #     fbank = freqm(fbank)\n",
    "        # if self.timem != 0:\n",
    "        #     fbank = timem(fbank)\n",
    "        # fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "        # normalize the input for both training and test\n",
    "        if not self.skip_norm:\n",
    "            fbank = (fbank - self.norm_mean) / (self.norm_std * 2)\n",
    "        # skip normalization the input if you are trying to get the normalization stats.\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # if self.noise == True:\n",
    "        #     fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * (self.noise_lvl**0.5)  # Add gaussian noise with configured noise level\n",
    "        #     fbank = torch.roll(fbank, np.random.randint(-10, 10), 0)\n",
    "\n",
    "        mix_ratio = min(mix_lambda, 1-mix_lambda) / \\\n",
    "            max(mix_lambda, 1-mix_lambda)\n",
    "\n",
    "        # the output fbank shape is [time_frame_num, frequency_bins], e.g., [1024, 128]\n",
    "        return fbank #, label_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------the evaluation dataloader---------------\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process speechcommands\n",
      "use dataset mean -6.846 and std 5.565 to normalize the input.\n",
      "number of classes is 6\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    AudioTestDataset(args.data_test, audio_conf=val_audio_conf),\n",
    "    batch_size=2, shuffle=False, num_workers=args.num_workers, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aacd9efd2e917f2085b49ad3eecd2bc8a974d0bb8b89bc48afae7fa44e9f517f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
