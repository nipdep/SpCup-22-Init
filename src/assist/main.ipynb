{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "from shutil import copy\n",
    "from typing import Dict, List, Union\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "from data_utils import (Dataset_ASVspoof2019_train,\n",
    "                        Dataset_ASVspoof2019_devNeval, genSpoof_list)\n",
    "from evaluation import calculate_tDCF_EER\n",
    "from utils import create_optimizer, seed_worker, set_seed, str_to_bool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "seed = 123\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config: Dict, device: torch.device):\n",
    "    \"\"\"Define DNN model architecture\"\"\"\n",
    "    module = import_module(\"models.{}\".format(model_config[\"architecture\"]))\n",
    "    _model = getattr(module, \"Model\")\n",
    "    model = _model(model_config).to(device)\n",
    "    nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "    print(\"no. model params:{}\".format(nb_params))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "        database_path: str,\n",
    "        label_path: str,\n",
    "        config: dict) -> List[torch.utils.data.DataLoader]:\n",
    "    \"\"\"Make PyTorch DataLoaders for train / developement / evaluation\"\"\"\n",
    "    # == dropping part ==\n",
    "    # track = config[\"track\"]\n",
    "    # prefix_2019 = \"ASVspoof2019.{}\".format(track)\n",
    "\n",
    "    # trn_database_path = database_path / \"ASVspoof2019_{}_train/\".format(track)\n",
    "    # dev_database_path = database_path / \"ASVspoof2019_{}_dev/\".format(track)\n",
    "    # eval_database_path = database_path / \"ASVspoof2019_{}_eval/\".format(track)\n",
    "\n",
    "    # trn_list_path = (database_path /\n",
    "    #                  \"ASVspoof2019_{}_cm_protocols/{}.cm.train.trn.txt\".format(\n",
    "    #                      track, prefix_2019))\n",
    "    # dev_trial_path = (database_path /\n",
    "    #                   \"ASVspoof2019_{}_cm_protocols/{}.cm.dev.trl.txt\".format(\n",
    "    #                       track, prefix_2019))\n",
    "    # eval_trial_path = (\n",
    "    #     database_path /\n",
    "    #     \"ASVspoof2019_{}_cm_protocols/{}.cm.eval.trl.txt\".format(\n",
    "    #         track, prefix_2019))\n",
    "\n",
    "    # d_label_trn, file_train = genSpoof_list(dir_meta=trn_list_path,\n",
    "    #                                         is_train=True,\n",
    "    #                                         is_eval=False)\n",
    "    # print(\"no. training files:\", len(file_train))\n",
    "    # =====================\n",
    "\n",
    "    label_df = pd.read_csv(label_path)\n",
    "    X, y = label_df['track'].values, label_df['algorithm'].values\n",
    "    # stratified split dataset into train-validation\n",
    "    # set param as config[\"split_ratio\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "\n",
    "    train_set = Dataset_ASVspoof2019_train(list_IDs=X_train,\n",
    "                                           labels=y_train,\n",
    "                                           base_dir=database_path)\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "    trn_loader = DataLoader(train_set,\n",
    "                            batch_size=config[\"batch_size\"],\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            pin_memory=True,\n",
    "                            worker_init_fn=seed_worker,\n",
    "                            generator=gen)\n",
    "\n",
    "    # # == test dataset not yet given ==\n",
    "\n",
    "    # _, file_dev = genSpoof_list(dir_meta=dev_trial_path,\n",
    "    #                             is_train=False,\n",
    "    #                             is_eval=False)\n",
    "    # print(\"no. validation files:\", len(file_dev))\n",
    "\n",
    "    # dev_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_dev,\n",
    "    #                                         base_dir=dev_database_path)\n",
    "    # dev_loader = DataLoader(dev_set,\n",
    "    #                         batch_size=config[\"batch_size\"],\n",
    "    #                         shuffle=False,\n",
    "    #                         drop_last=False,\n",
    "    #                         pin_memory=True)\n",
    "    # =================================\n",
    "\n",
    "    # == validation dataset {updated} == \n",
    "\n",
    "    eval_set = Dataset_ASVspoof2019_train(list_IDs=X_test,\n",
    "                                           labels=y_test,\n",
    "                                           base_dir=database_path)\n",
    "    eval_loader = DataLoader(eval_set,\n",
    "                             batch_size=config[\"batch_size\"],\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             pin_memory=True,\n",
    "                             worker_init_fn=seed_worker,\n",
    "                             generator=gen)\n",
    "\n",
    "    return trn_loader, eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_evaluation_file(\n",
    "    data_loader: DataLoader,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    save_path: str,\n",
    "    trial_path: str) -> None:\n",
    "    \"\"\"Perform evaluation and save the score to a file\"\"\"\n",
    "    model.eval()\n",
    "    with open(trial_path, \"r\") as f_trl:\n",
    "        trial_lines = f_trl.readlines()\n",
    "    fname_list = []\n",
    "    score_list = []\n",
    "    for batch_x, utt_id in data_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, batch_out = model(batch_x)\n",
    "            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "\n",
    "    assert len(trial_lines) == len(fname_list) == len(score_list)\n",
    "    with open(save_path, \"w\") as fh:\n",
    "        for fn, sco, trl in zip(fname_list, score_list, trial_lines):\n",
    "            _, utt_id, _, src, key = trl.strip().split(' ')\n",
    "            assert fn == utt_id\n",
    "            fh.write(\"{} {} {} {}\\n\".format(utt_id, src, key, sco))\n",
    "    print(\"Scores saved to {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    trn_loader: DataLoader,\n",
    "    model,\n",
    "    optim: Union[torch.optim.SGD, torch.optim.Adam],\n",
    "    device: torch.device,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    config: argparse.Namespace):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    running_loss = 0\n",
    "    num_total = 0.0\n",
    "    ii = 0\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # set objective (Loss) functions\n",
    "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
    "    # criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_x, batch_y in trn_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        ii += 1\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_y = batch_y.view(-1).to(device)\n",
    "        _, batch_out = model(batch_x)#, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "        running_loss += batch_loss.item() * batch_size\n",
    "        optim.zero_grad()\n",
    "        scaler.scale(batch_loss).backward()\n",
    "        # batch_loss.backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        # optim.step()\n",
    "\n",
    "        if config[\"optim_config\"][\"scheduler\"] in [\"cosine\", \"keras_decay\"]:\n",
    "            scheduler.step()\n",
    "        elif scheduler is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"scheduler error, got:{}\".format(scheduler))\n",
    "\n",
    "    running_loss /= num_total\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    trn_loader: DataLoader,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    config: argparse.Namespace):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    running_loss = 0\n",
    "    num_total = 0.0\n",
    "    ii = 0\n",
    "    model.eval()\n",
    "\n",
    "    # set objective (Loss) functions\n",
    "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
    "    # criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_x, batch_y in trn_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        ii += 1\n",
    "        batch_x = batch_x.to(device)\n",
    "        # batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_y = batch_y.view(-1).to(device)\n",
    "        _, batch_out = model(batch_x, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "        running_loss += batch_loss.item() * batch_size\n",
    "        \n",
    "    running_loss /= num_total\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"database_path\": \"../../data/spcup_2022_training_part1/\",\n",
    "    \"label_path\": \"../../data/spcup_2022_training_part1/labels.csv\",\n",
    "    \"asv_score_path\": \"ASVspoof2019_LA_asv_scores/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt\",\n",
    "    \"model_path\": \"./models/weights/AASIST.pth\",\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 100,\n",
    "    \"loss\": \"CCE\",\n",
    "    \"track\": \"LA\",\n",
    "    \"eval_all_best\": \"True\",\n",
    "    \"eval_output\": \"eval_scores_using_best_dev_model.txt\",\n",
    "    \"cudnn_deterministic_toggle\": \"True\",\n",
    "    \"cudnn_benchmark_toggle\": \"False\",\n",
    "    \"model_config\": {\n",
    "        \"architecture\": \"AASIST\",\n",
    "        \"nb_samp\": 64600,\n",
    "        \"first_conv\": 128,\n",
    "        \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]],\n",
    "        \"gat_dims\": [64, 32],\n",
    "        \"pool_ratios\": [0.5, 0.7, 0.5, 0.5],\n",
    "        \"temperatures\": [2.0, 2.0, 100.0, 100.0]\n",
    "    },\n",
    "    \"optim_config\": {\n",
    "        \"optimizer\": \"adam\", \n",
    "        \"amsgrad\": \"False\",\n",
    "        \"base_lr\": 0.0001,\n",
    "        \"lr_min\": 0.000005,\n",
    "        \"betas\": [0.9, 0.999],\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"scheduler\": \"cosine\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.config = config\n",
    "args.seed = seed\n",
    "args.output_dir = '../../tmp/out'\n",
    "args.comment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    # define database related paths\n",
    "    output_dir = Path(args.output_dir)\n",
    "    database_path = Path(config[\"database_path\"])\n",
    "    label_path = Path(config['label_path'])\n",
    "\n",
    "    # set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device: {}\".format(device))\n",
    "    # if device == \"cpu\":\n",
    "    #     raise ValueError(\"GPU not detected!\")\n",
    "\n",
    "    # define model architecture\n",
    "    model_config = args.config[\"model_config\"]\n",
    "    model = get_model(model_config, device)\n",
    "\n",
    "    # define dataloaders\n",
    "    trn_loader, eval_loader = get_loader(database_path, label_path, config)\n",
    "\n",
    "    # get optimizer and scheduler\n",
    "    optim_config = config[\"optim_config\"]\n",
    "    optim_config[\"epochs\"] = config[\"num_epochs\"]\n",
    "    optim_config[\"steps_per_epoch\"] = len(trn_loader)\n",
    "    optimizer, scheduler = create_optimizer(model.parameters(), optim_config)\n",
    "    optimizer_swa = SWA(optimizer)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(\"Start training epoch{:03d}\".format(epoch))\n",
    "        training_loss = train_epoch(trn_loader, model, optimizer, device,\n",
    "                                   scheduler, config)\n",
    "        eval_loss = eval_epoch(eval_loader, model, device, config)\n",
    "        print(f'[{epoch}] Training Loss : {training_loss} | Eval Loss : {eval_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "no. model params:298349\n",
      "Start training epoch000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 4.00 GiB total capacity; 2.71 GiB already allocated; 0 bytes free; 2.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13144/1730160679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13144/979886145.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training epoch{:03d}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         training_loss = train_epoch(trn_loader, model, optimizer, device,\n\u001b[0m\u001b[0;32m     32\u001b[0m                                    scheduler, config)\n\u001b[0;32m     33\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13144/3636883822.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(trn_loader, model, optim, device, scheduler, config)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, Freq_aug=str_to_bool(config[\"freq_aug\"]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Data Science\\Projects\\SP cup 2022\\SpCup-22-Init\\src\\assist\\models\\AASIST.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, Freq_aug)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;31m# get embeddings using encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;31m# (#bs, #filt, #spec, #seq)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;31m# spectral GAT (GAT-S)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Data Science\\Projects\\SP cup 2022\\SpCup-22-Init\\src\\assist\\models\\AASIST.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;31m# print('out',out.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;31m# print('out',out.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \"\"\"\n\u001b[1;32m--> 168\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2282\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2284\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 4.00 GiB total capacity; 2.71 GiB already allocated; 0 bytes free; 2.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Main function.\n",
    "    Trains, validates, and evaluates the ASVspoof detection model.\n",
    "    \"\"\"\n",
    "    # load experiment configurations\n",
    "    with open(args.config, \"r\") as f_json:\n",
    "        config = json.loads(f_json.read())\n",
    "    model_config = config[\"model_config\"]\n",
    "    optim_config = config[\"optim_config\"]\n",
    "    optim_config[\"epochs\"] = config[\"num_epochs\"]\n",
    "    track = config[\"track\"]\n",
    "    assert track in [\"LA\", \"PA\", \"DF\"], \"Invalid track given\"\n",
    "    if \"eval_all_best\" not in config:\n",
    "        config[\"eval_all_best\"] = \"True\"\n",
    "    if \"freq_aug\" not in config:\n",
    "        config[\"freq_aug\"] = \"False\"\n",
    "\n",
    "    # make experiment reproducible\n",
    "    set_seed(args.seed, config)\n",
    "\n",
    "    # define database related paths\n",
    "    output_dir = Path(args.output_dir)\n",
    "    prefix_2019 = \"ASVspoof2019.{}\".format(track)\n",
    "    database_path = Path(config[\"database_path\"])\n",
    "    label_path = Path(config['label_path'])\n",
    "\n",
    "    # define model related paths\n",
    "    model_tag = \"{}_{}_ep{}_bs{}\".format(\n",
    "        track,\n",
    "        os.path.splitext(os.path.basename(args.config))[0],\n",
    "        config[\"num_epochs\"], config[\"batch_size\"])\n",
    "    if args.comment:\n",
    "        model_tag = model_tag + \"_{}\".format(args.comment)\n",
    "    model_tag = output_dir / model_tag\n",
    "    model_save_path = model_tag / \"weights\"\n",
    "    eval_score_path = model_tag / config[\"eval_output\"]\n",
    "    if False:\n",
    "        writer = SummaryWriter(model_tag)\n",
    "\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    copy(args.config, model_tag / \"config.conf\")\n",
    "\n",
    "    # set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device: {}\".format(device))\n",
    "    if device == \"cpu\":\n",
    "        raise ValueError(\"GPU not detected!\")\n",
    "\n",
    "    # define model architecture\n",
    "    model = get_model(model_config, device)\n",
    "\n",
    "    # define dataloaders\n",
    "    trn_loader, dev_loader = get_loader(database_path, label_path, config)\n",
    "\n",
    "    # == evaluating model ==\n",
    "    # evaluates pretrained model and exit script\n",
    "    # if args.eval:\n",
    "    #     model.load_state_dict(\n",
    "    #         torch.load(config[\"model_path\"], map_location=device))\n",
    "    #     print(\"Model loaded : {}\".format(config[\"model_path\"]))\n",
    "    #     print(\"Start evaluation...\")\n",
    "    #     produce_evaluation_file(eval_loader, model, device,\n",
    "    #                             eval_score_path, eval_trial_path)\n",
    "    #     calculate_tDCF_EER(cm_scores_file=eval_score_path,\n",
    "    #                        asv_score_file=database_path /\n",
    "    #                        config[\"asv_score_path\"],\n",
    "    #                        output_file=model_tag / \"t-DCF_EER.txt\")\n",
    "    #     print(\"DONE.\")\n",
    "    #     eval_eer, eval_tdcf = calculate_tDCF_EER(\n",
    "    #         cm_scores_file=eval_score_path,\n",
    "    #         asv_score_file=database_path / config[\"asv_score_path\"],\n",
    "    #         output_file=model_tag/\"loaded_model_t-DCF_EER.txt\")\n",
    "    #     sys.exit(0)\n",
    "    # ======================\n",
    "\n",
    "    # get optimizer and scheduler\n",
    "    optim_config[\"steps_per_epoch\"] = len(trn_loader)\n",
    "    optimizer, scheduler = create_optimizer(model.parameters(), optim_config)\n",
    "    optimizer_swa = SWA(optimizer)\n",
    "\n",
    "    best_dev_eer = 1.\n",
    "    best_eval_eer = 100.\n",
    "    best_dev_tdcf = 0.05\n",
    "    best_eval_tdcf = 1.\n",
    "    n_swa_update = 0  # number of snapshots of model to use in SWA\n",
    "    f_log = open(model_tag / \"metric_log.txt\", \"a\")\n",
    "    f_log.write(\"=\" * 5 + \"\\n\")\n",
    "\n",
    "    # make directory for metric logging\n",
    "    metric_path = model_tag / \"metrics\"\n",
    "    os.makedirs(metric_path, exist_ok=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(\"Start training epoch{:03d}\".format(epoch))\n",
    "        running_loss = train_epoch(trn_loader, model, optimizer, device,\n",
    "                                   scheduler, config)\n",
    "        produce_evaluation_file(dev_loader, model, device,\n",
    "                                metric_path/\"dev_score.txt\", dev_trial_path)\n",
    "        dev_eer, dev_tdcf = calculate_tDCF_EER(\n",
    "            cm_scores_file=metric_path/\"dev_score.txt\",\n",
    "            asv_score_file=database_path/config[\"asv_score_path\"],\n",
    "            output_file=metric_path/\"dev_t-DCF_EER_{}epo.txt\".format(epoch),\n",
    "            printout=False)\n",
    "        print(\"DONE.\\nLoss:{:.5f}, dev_eer: {:.3f}, dev_tdcf:{:.5f}\".format(\n",
    "            running_loss, dev_eer, dev_tdcf))\n",
    "        \n",
    "        if False:\n",
    "            writer.add_scalar(\"loss\", running_loss, epoch)\n",
    "            writer.add_scalar(\"dev_eer\", dev_eer, epoch)\n",
    "            writer.add_scalar(\"dev_tdcf\", dev_tdcf, epoch)\n",
    "\n",
    "        best_dev_tdcf = min(dev_tdcf, best_dev_tdcf)\n",
    "        if best_dev_eer >= dev_eer:\n",
    "            print(\"best model find at epoch\", epoch)\n",
    "            best_dev_eer = dev_eer\n",
    "            torch.save(model.state_dict(),\n",
    "                       model_save_path / \"epoch_{}_{:03.3f}.pth\".format(epoch, dev_eer))\n",
    "\n",
    "            # do evaluation whenever best model is renewed\n",
    "            if str_to_bool(config[\"eval_all_best\"]):\n",
    "                produce_evaluation_file(eval_loader, model, device,\n",
    "                                        eval_score_path, eval_trial_path)\n",
    "                eval_eer, eval_tdcf = calculate_tDCF_EER(\n",
    "                    cm_scores_file=eval_score_path,\n",
    "                    asv_score_file=database_path / config[\"asv_score_path\"],\n",
    "                    output_file=metric_path /\n",
    "                    \"t-DCF_EER_{:03d}epo.txt\".format(epoch))\n",
    "\n",
    "                log_text = \"epoch{:03d}, \".format(epoch)\n",
    "                if eval_eer < best_eval_eer:\n",
    "                    log_text += \"best eer, {:.4f}%\".format(eval_eer)\n",
    "                    best_eval_eer = eval_eer\n",
    "                if eval_tdcf < best_eval_tdcf:\n",
    "                    log_text += \"best tdcf, {:.4f}\".format(eval_tdcf)\n",
    "                    best_eval_tdcf = eval_tdcf\n",
    "                    torch.save(model.state_dict(),\n",
    "                               model_save_path / \"best.pth\")\n",
    "                if len(log_text) > 0:\n",
    "                    print(log_text)\n",
    "                    f_log.write(log_text + \"\\n\")\n",
    "\n",
    "            print(\"Saving epoch {} for swa\".format(epoch))\n",
    "            optimizer_swa.update_swa()\n",
    "            n_swa_update += 1\n",
    "        \n",
    "        if False:\n",
    "            writer.add_scalar(\"best_dev_eer\", best_dev_eer, epoch)\n",
    "            writer.add_scalar(\"best_dev_tdcf\", best_dev_tdcf, epoch)\n",
    "\n",
    "    print(\"Start final evaluation\")\n",
    "    epoch += 1\n",
    "    if n_swa_update > 0:\n",
    "        optimizer_swa.swap_swa_sgd()\n",
    "        optimizer_swa.bn_update(trn_loader, model, device=device)\n",
    "    produce_evaluation_file(eval_loader, model, device, eval_score_path,\n",
    "                            eval_trial_path)\n",
    "    eval_eer, eval_tdcf = calculate_tDCF_EER(cm_scores_file=eval_score_path,\n",
    "                                             asv_score_file=database_path /\n",
    "                                             config[\"asv_score_path\"],\n",
    "                                             output_file=model_tag / \"t-DCF_EER.txt\")\n",
    "    f_log = open(model_tag / \"metric_log.txt\", \"a\")\n",
    "    f_log.write(\"=\" * 5 + \"\\n\")\n",
    "    f_log.write(\"EER: {:.3f}, min t-DCF: {:.5f}\".format(eval_eer, eval_tdcf))\n",
    "    f_log.close()\n",
    "\n",
    "    torch.save(model.state_dict(),\n",
    "               model_save_path / \"swa.pth\")\n",
    "\n",
    "    if eval_eer <= best_eval_eer:\n",
    "        best_eval_eer = eval_eer\n",
    "    if eval_tdcf <= best_eval_tdcf:\n",
    "        best_eval_tdcf = eval_tdcf\n",
    "        torch.save(model.state_dict(),\n",
    "                   model_save_path / \"best.pth\")\n",
    "    print(\"Exp FIN. EER: {:.3f}, min t-DCF: {:.5f}\".format(\n",
    "        best_eval_eer, best_eval_tdcf))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4342e088104506c762c0bda77e093724389a62f7580ab4d8d79c756006761a67"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('wave_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
