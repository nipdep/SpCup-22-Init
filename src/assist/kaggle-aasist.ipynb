{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install torchcontrib","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:48:42.475642Z","iopub.execute_input":"2022-02-26T16:48:42.476546Z","iopub.status.idle":"2022-02-26T16:48:53.643429Z","shell.execute_reply.started":"2022-02-26T16:48:42.476428Z","shell.execute_reply":"2022-02-26T16:48:53.642618Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport json\nimport os\nimport sys\nimport warnings\nfrom importlib import import_module\nfrom pathlib import Path\nfrom shutil import copy\nfrom typing import Dict, List, Union\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\n# from torch.utils.tensorboard import SummaryWriter\nfrom torchcontrib.optim import SWA\n\nfrom data_utils import (Dataset_ASVspoof2019_train,\n                        Dataset_ASVspoof2019_devNeval, genSpoof_list)\nfrom evaluation import calculate_tDCF_EER\nfrom utilsa import create_optimizer, seed_worker, set_seed, str_to_bool\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nseed = 123\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:48:53.647248Z","iopub.execute_input":"2022-02-26T16:48:53.647467Z","iopub.status.idle":"2022-02-26T16:48:55.893836Z","shell.execute_reply.started":"2022-02-26T16:48:53.647442Z","shell.execute_reply":"2022-02-26T16:48:55.892764Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"***\n__File Operations__","metadata":{}},{"cell_type":"code","source":"! wget \"https://www.dropbox.com/s/36yqmymkva2bwdi/spcup_2022_training_part1.zip?dl=1\" -c -O 'spcup_2022_training_part1.zip'\n! wget \"https://www.dropbox.com/s/wsmlthhri29fb79/spcup_2022_unseen.zip?dl=1\" -c -O 'spcup_2022_unseen.zip'","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:48:55.895844Z","iopub.execute_input":"2022-02-26T16:48:55.896132Z","iopub.status.idle":"2022-02-26T16:49:34.631512Z","shell.execute_reply.started":"2022-02-26T16:48:55.896094Z","shell.execute_reply":"2022-02-26T16:49:34.630650Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!unzip \"./spcup_2022_training_part1.zip\" -d \"./spcup_2022_training/\"\n!unzip \"./spcup_2022_unseen.zip\" -d \"./spcup_2022_unseen/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm \"./spcup_2022_training_part1.zip\"\n!rm \"./spcup_2022_unseen.zip\"","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:51.595102Z","iopub.execute_input":"2022-02-26T16:49:51.595532Z","iopub.status.idle":"2022-02-26T16:49:53.136393Z","shell.execute_reply.started":"2022-02-26T16:49:51.595492Z","shell.execute_reply":"2022-02-26T16:49:53.135383Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# create combined label df\n# combine two label sets\ndf1 = pd.read_csv('./spcup_2022_training/spcup_2022_training_part1/labels.csv')\ndf2 = pd.read_csv('./spcup_2022_unseen/spcup_2022_unseen/labels.csv')\ndf3 = pd.concat([df1, df2]).sample(frac=1)\n\ndf3.to_csv('./final_labels.csv', index=False)\n!rm './spcup_2022_unseen/spcup_2022_unseen/labels.csv'","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:53.138139Z","iopub.execute_input":"2022-02-26T16:49:53.138404Z","iopub.status.idle":"2022-02-26T16:49:54.001372Z","shell.execute_reply.started":"2022-02-26T16:49:53.138374Z","shell.execute_reply":"2022-02-26T16:49:54.000391Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# copy .wav files from unseen to all\n!cp -a \"./spcup_2022_unseen/spcup_2022_unseen/\". \"./spcup_2022_training/spcup_2022_training_part1/\"","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:54.003330Z","iopub.execute_input":"2022-02-26T16:49:54.003618Z","iopub.status.idle":"2022-02-26T16:49:54.982107Z","shell.execute_reply.started":"2022-02-26T16:49:54.003580Z","shell.execute_reply":"2022-02-26T16:49:54.981037Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"def get_model(model_config: Dict, device: torch.device):\n    \"\"\"Define DNN model architecture\"\"\"\n    module = import_module(\"{}\".format(model_config[\"architecture\"]))\n    _model = getattr(module, \"Model\")\n    model = _model(model_config).to(device)\n    nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n    print(\"no. model params:{}\".format(nb_params))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:54.984091Z","iopub.execute_input":"2022-02-26T16:49:54.984669Z","iopub.status.idle":"2022-02-26T16:49:55.195230Z","shell.execute_reply.started":"2022-02-26T16:49:54.984624Z","shell.execute_reply":"2022-02-26T16:49:55.194470Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_loader(\n        database_path: str,\n        label_path: str,\n        config: dict) -> List[torch.utils.data.DataLoader]:\n    \"\"\"Make PyTorch DataLoaders for train / developement / evaluation\"\"\"\n    # == dropping part ==\n    # track = config[\"track\"]\n    # prefix_2019 = \"ASVspoof2019.{}\".format(track)\n\n    # trn_database_path = database_path / \"ASVspoof2019_{}_train/\".format(track)\n    # dev_database_path = database_path / \"ASVspoof2019_{}_dev/\".format(track)\n    # eval_database_path = database_path / \"ASVspoof2019_{}_eval/\".format(track)\n\n    # trn_list_path = (database_path /\n    #                  \"ASVspoof2019_{}_cm_protocols/{}.cm.train.trn.txt\".format(\n    #                      track, prefix_2019))\n    # dev_trial_path = (database_path /\n    #                   \"ASVspoof2019_{}_cm_protocols/{}.cm.dev.trl.txt\".format(\n    #                       track, prefix_2019))\n    # eval_trial_path = (\n    #     database_path /\n    #     \"ASVspoof2019_{}_cm_protocols/{}.cm.eval.trl.txt\".format(\n    #         track, prefix_2019))\n\n    # d_label_trn, file_train = genSpoof_list(dir_meta=trn_list_path,\n    #                                         is_train=True,\n    #                                         is_eval=False)\n    # print(\"no. training files:\", len(file_train))\n    # =====================\n\n    label_df = pd.read_csv(label_path)\n    X, y = label_df['track'].values, label_df['algorithm'].values\n    # stratified split dataset into train-validation\n    # set param as config[\"split_ratio\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n\n\n    train_set = Dataset_ASVspoof2019_train(list_IDs=X_train,\n                                           labels=y_train,\n                                           base_dir=database_path)\n    gen = torch.Generator()\n    gen.manual_seed(seed)\n    trn_loader = DataLoader(train_set,\n                            batch_size=config[\"batch_size\"],\n                            shuffle=True,\n                            drop_last=True,\n                            pin_memory=True,\n                            worker_init_fn=seed_worker,\n                            generator=gen)\n\n    # # == test dataset not yet given ==\n\n    # _, file_dev = genSpoof_list(dir_meta=dev_trial_path,\n    #                             is_train=False,\n    #                             is_eval=False)\n    # print(\"no. validation files:\", len(file_dev))\n\n    # dev_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_dev,\n    #                                         base_dir=dev_database_path)\n    # dev_loader = DataLoader(dev_set,\n    #                         batch_size=config[\"batch_size\"],\n    #                         shuffle=False,\n    #                         drop_last=False,\n    #                         pin_memory=True)\n    # =================================\n\n    # == validation dataset {updated} == \n\n    eval_set = Dataset_ASVspoof2019_train(list_IDs=X_test,\n                                           labels=y_test,\n                                           base_dir=database_path)\n    eval_loader = DataLoader(eval_set,\n                             batch_size=config[\"batch_size\"],\n                             shuffle=False,\n                             drop_last=False,\n                             pin_memory=True,\n                             worker_init_fn=seed_worker,\n                             generator=gen)\n\n    return trn_loader, eval_loader","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:55.199435Z","iopub.execute_input":"2022-02-26T16:49:55.203059Z","iopub.status.idle":"2022-02-26T16:49:56.006037Z","shell.execute_reply.started":"2022-02-26T16:49:55.201540Z","shell.execute_reply":"2022-02-26T16:49:56.005231Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def produce_evaluation_file(\n    data_loader: DataLoader,\n    model,\n    device: torch.device,\n    save_path: str,\n    trial_path: str) -> None:\n    \"\"\"Perform evaluation and save the score to a file\"\"\"\n    model.eval()\n    with open(trial_path, \"r\") as f_trl:\n        trial_lines = f_trl.readlines()\n    fname_list = []\n    score_list = []\n    for batch_x, utt_id in data_loader:\n        batch_x = batch_x.to(device)\n        with torch.no_grad():\n            _, batch_out = model(batch_x)\n            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n        # add outputs\n        fname_list.extend(utt_id)\n        score_list.extend(batch_score.tolist())\n\n    assert len(trial_lines) == len(fname_list) == len(score_list)\n    with open(save_path, \"w\") as fh:\n        for fn, sco, trl in zip(fname_list, score_list, trial_lines):\n            _, utt_id, _, src, key = trl.strip().split(' ')\n            assert fn == utt_id\n            fh.write(\"{} {} {} {}\\n\".format(utt_id, src, key, sco))\n    print(\"Scores saved to {}\".format(save_path))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.009493Z","iopub.execute_input":"2022-02-26T16:49:56.009823Z","iopub.status.idle":"2022-02-26T16:49:56.019565Z","shell.execute_reply.started":"2022-02-26T16:49:56.009782Z","shell.execute_reply":"2022-02-26T16:49:56.018610Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train_epoch(\n    trn_loader: DataLoader,\n    model,\n    optim: Union[torch.optim.SGD, torch.optim.Adam],\n    device: torch.device,\n    scheduler: torch.optim.lr_scheduler,\n    config: argparse.Namespace):\n    \"\"\"Train the model for one epoch\"\"\"\n    \n    running_loss = 0\n    num_total = 0.0\n    train_acc, correct_train, target_count = 0, 0, 0\n    ii = 0\n    model.train()\n    scaler = GradScaler()\n\n    # set objective (Loss) functions\n    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n    # criterion = nn.CrossEntropyLoss(weight=weight)\n    criterion = nn.CrossEntropyLoss()\n    for batch_x, batch_y in trn_loader:\n        batch_size = batch_x.size(0)\n        num_total += batch_size\n        ii += 1\n        batch_x = batch_x.to(device)\n        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n        batch_y = batch_y.view(-1).to(device)\n        _, batch_out = model(batch_x)#, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n        batch_loss = criterion(batch_out, batch_y)\n        running_loss += batch_loss.item() * batch_size\n        optim.zero_grad()\n        scaler.scale(batch_loss).backward()\n        # batch_loss.backward()\n        scaler.step(optim)\n        scaler.update()\n        # optim.step()\n\n        if config[\"optim_config\"][\"scheduler\"] in [\"cosine\", \"keras_decay\"]:\n            scheduler.step()\n        elif scheduler is None:\n            pass\n        else:\n            raise ValueError(\"scheduler error, got:{}\".format(scheduler))\n        \n        # accuracy\n        _, predicted = torch.max(batch_out.data, 1)\n        target_count += batch_y.size(0)\n        correct_train += (batch_y == predicted).sum().item()\n        train_acc = (100 * correct_train) / target_count\n\n    running_loss /= num_total\n    return running_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.021410Z","iopub.execute_input":"2022-02-26T16:49:56.021682Z","iopub.status.idle":"2022-02-26T16:49:56.035960Z","shell.execute_reply.started":"2022-02-26T16:49:56.021646Z","shell.execute_reply":"2022-02-26T16:49:56.035086Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def eval_epoch(\n    trn_loader: DataLoader,\n    model,\n    device: torch.device,\n    config: argparse.Namespace):\n    \"\"\"Train the model for one epoch\"\"\"\n    running_loss = 0\n    num_total = 0.0\n    val_acc, correct_train, target_count = 0, 0, 0\n    ii = 0\n    model.eval()\n\n    # set objective (Loss) functions\n    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n    # criterion = nn.CrossEntropyLoss(weight=weight)\n    criterion = nn.CrossEntropyLoss()\n    for batch_x, batch_y in trn_loader:\n        batch_size = batch_x.size(0)\n        num_total += batch_size\n        ii += 1\n        batch_x = batch_x.to(device)\n        # batch_y = batch_y.view(-1).type(torch.int64).to(device)\n        batch_y = batch_y.view(-1).to(device)\n        _, batch_out = model(batch_x) #model(batch_x, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n        batch_loss = criterion(batch_out, batch_y)\n        running_loss += batch_loss.item() * batch_size\n        \n        # accuracy\n        _, predicted = torch.max(batch_out.data, 1)\n        target_count += batch_y.size(0)\n        correct_train += (batch_y == predicted).sum().item()\n        val_acc = (100 * correct_train) / target_count\n        \n    running_loss /= num_total\n    return running_loss, val_acc","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.037531Z","iopub.execute_input":"2022-02-26T16:49:56.038105Z","iopub.status.idle":"2022-02-26T16:49:56.049022Z","shell.execute_reply.started":"2022-02-26T16:49:56.038065Z","shell.execute_reply":"2022-02-26T16:49:56.048136Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"config = {\n    \n    \"database_path\": \"./spcup_2022_training/spcup_2022_training_part1/\",\n    \"label_path\": \"./final_labels.csv\",\n    \"asv_score_path\": \"ASVspoof2019_LA_asv_scores/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt\",\n    \"model_path\": \"./models/weights/AASIST.pth\",\n    \"batch_size\": 8,  # cloud not use higher value, because of CUDA memory overflow\n    \"num_epochs\": 100,\n    \"loss\": \"CCE\",\n    \"track\": \"LA\",\n    \"eval_all_best\": \"True\",\n    \"eval_output\": \"eval_scores_using_best_dev_model.txt\",\n    \"cudnn_deterministic_toggle\": \"True\",\n    \"cudnn_benchmark_toggle\": \"False\",\n    \"model_config\": {\n        \"architecture\": \"aasist\",\n        \"nb_samp\": 64600,\n        \"first_conv\": 128,\n        \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]],\n        \"gat_dims\": [64, 32],\n        \"pool_ratios\": [0.5, 0.7, 0.5, 0.5],\n        \"temperatures\": [2.0, 2.0, 100.0, 100.0]\n    },\n    \"optim_config\": {\n        \"optimizer\": \"adam\", \n        \"amsgrad\": \"False\",\n        \"base_lr\": 0.0001,\n        \"lr_min\": 0.000005,\n        \"betas\": [0.9, 0.999],\n        \"weight_decay\": 0.0001,\n        \"scheduler\": \"cosine\"\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.050697Z","iopub.execute_input":"2022-02-26T16:49:56.051005Z","iopub.status.idle":"2022-02-26T16:49:56.061655Z","shell.execute_reply.started":"2022-02-26T16:49:56.050963Z","shell.execute_reply":"2022-02-26T16:49:56.060941Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"args = argparse.Namespace()\nargs.config = config\nargs.seed = seed\nargs.output_dir = './tmp/out'\nargs.comment = False","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.063411Z","iopub.execute_input":"2022-02-26T16:49:56.063959Z","iopub.status.idle":"2022-02-26T16:49:56.072729Z","shell.execute_reply.started":"2022-02-26T16:49:56.063921Z","shell.execute_reply":"2022-02-26T16:49:56.072063Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def main(args):\n\n    # define database related paths\n    output_dir = Path(args.output_dir)\n    database_path = Path(config[\"database_path\"])\n    label_path = Path(config['label_path'])\n\n    # set device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Device: {}\".format(device))\n    # if device == \"cpu\":\n    #     raise ValueError(\"GPU not detected!\")\n\n    # define model architecture\n    model_config = args.config[\"model_config\"]\n    model = get_model(model_config, device)\n\n    # define dataloaders\n    trn_loader, eval_loader = get_loader(database_path, label_path, config)\n\n    # get optimizer and scheduler\n    optim_config = config[\"optim_config\"]\n    optim_config[\"epochs\"] = config[\"num_epochs\"]\n    optim_config[\"steps_per_epoch\"] = len(trn_loader)\n    optimizer, scheduler = create_optimizer(model.parameters(), optim_config)\n    optimizer_swa = SWA(optimizer)\n\n    # Training\n    for epoch in range(config[\"num_epochs\"]):\n        print(\"Start training epoch{:03d}\".format(epoch))\n        training_loss, training_acc = train_epoch(trn_loader, model, optimizer, device,\n                                   scheduler, config)\n        eval_loss, eval_acc = eval_epoch(eval_loader, model, device, config)\n        print(f'[{epoch}] Training Loss : {training_loss} / Training Accuracy : {training_acc} | Eval Loss : {eval_loss} / Eval Accuracy : {eval_acc}')","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.073706Z","iopub.execute_input":"2022-02-26T16:49:56.073916Z","iopub.status.idle":"2022-02-26T16:49:56.084883Z","shell.execute_reply.started":"2022-02-26T16:49:56.073884Z","shell.execute_reply":"2022-02-26T16:49:56.084222Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"main(args)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:49:56.086256Z","iopub.execute_input":"2022-02-26T16:49:56.087004Z","iopub.status.idle":"2022-02-26T17:38:27.513899Z","shell.execute_reply.started":"2022-02-26T16:49:56.086967Z","shell.execute_reply":"2022-02-26T17:38:27.512128Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}